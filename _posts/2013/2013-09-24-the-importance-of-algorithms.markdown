---
layout: post
title: 论算法的重要[译]
date: 2013-09-24 19:42:23
categories:
- 技术流/techgangster
tags:
- algorithm
- 翻译
- 做人要讲道理
---

**算法其实不是很重要，起码对大部分人来说是这样。因为总有极个别聪明的家伙发明算法，也有部分机智的家伙应用算法，根本不需要剩下的人操心，甚至透明到他们都不知道这个世界还有算法存在。所以话说回来，你是想做大部分人，还是想成为那个特别的人？**

**这篇文章翻译自TopCoder上的这篇[_Importance of Algorithms_](http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=importance_of_algorithms)。老子吐血翻译，然后居然翻译到一半意识到忘记用繁体字了=。=然后就这样吧。转载请注明出处就好谢谢。**

---

**介绍**

想要理解为什么算法研究与算法知识如此重要，首先我们要准确的定义到底什么是算法。

在流行的算法教科书 [_算法导论(Introduction to Algorithms)_](http://book.douban.com/subject/3904676/) 中的定义：“一个算法是指清晰明了的计算程序，将数据或者一组的数据作为入参，并能得出数据或者一组的数据作为出参结果。”换而言之，算法对于一个明确给定的待解决任务来说，就如同路线图一般。比如一坨计算费布那切(Fibonacci)数列的程序就是一个典型的算法实现。更甚至，计算两数加法的程序虽然简单，但是算是一种算法实现。

一些算法，比如费布那切数列的计算，都是深嵌着我们的的逻辑思维以及解决问题的技巧的。对大多数人而言，学习复杂的算法，是更高效更有逻辑解决问题的基石。实际上你应该会很惊奇的发现很多复杂的算法在人们日常用计算机检查邮件以及听音乐时频繁被使用到。这篇文章将会介绍一些与算法分析相关的基础思想，并会用例子来说明为什么对算法的学习是很重要的。

**执行时间的分析**

说明一个算法牛逼的重要的切入点就是它有多快。通常很容易想出一个算法解决问题，但是如果它太慢了，就得重新回到构思中。算法的速度依赖于算法的执行环境，也依赖于算法的具体实现，计算机科学家会说运行时间与输入数据的大小有关。比如说，输入N个整数，算法的执行时间可能是N^2 增长，可以用O(N^2 )表示。这意味着如果你用在计算机上实现的算法输入N个数据，那么它会需要C*N^2 秒来得出结果。这里的C代表一个不会随着输入大小而改变的常量。

然而，许多复杂算法的执行时间会被除输入量级以外的其他因素影响。例如，一个排序算法在处理给定输入的整数集是已排序时的速度会比随机顺序的整数集快很多。所以经常会听到“最坏用例运行时间”或者“平均用例运行时间”。最坏用例执行时间是指所有给定数据中最阴险狡诈的那个输入。平均用例执行时间则是所有给定输入数据的平均执行时间。所以最坏用例执行时间才能反映出问题，所以经常用作算法的校验标准。决定哪个是最坏用例以及平均用例的过程还是很有技巧的，因为不可能用算法把所有可能的输入都运行一遍。很多很好的在线资源可以帮你评估这些数据。

算法的执行时间近似换算表，N = 100

|			|			|
|-----------|----------|
| O(Log(N)) | 10^-7 秒 
| O(N)      | 10^-6 秒 
| O(N*Log(N)) | 10^-5 秒
| O(N^2 )   | 10^-4 秒 
| O(N^6)    | 3分钟    
| O(2^N)    | 10^14 年 
| O(N!)     |  10^142 年
|           |          



**排序算法**

排序算法是计算机科学家经常用到的一种算法，所以是很好的范例。最简单的排序是把一堆元素中最小的那个元素抽出放在第一位，然后取出第二小的，放在第二位，以此类推。不幸的是，这种方法的时间复杂度为O(N^2 )，这意味着运行时间与元素数量的平方成正比。如果你要排十亿数量级的元素集合，这这种算法需要10^18 次运算。让这个更有既视感些的说法，就是一台个人计算机每秒能执行10^9 次运算，所以10^18 次需要耗费1902年(10^18 / (10^9 \*60\*24*365))的时间。

还好，有一坨的优秀算法(快排[quicksort]，堆排[heapsort]，合并排序[mergesort]等)被实现了，许多实现的时间复杂度都为O(N*Log(N))。这把排序十亿量级的元素集合的操作数降到了个人计算机运算可接受的范围。从10^18 降到10^10 ， **1亿倍**的加速。

**最短路径**

研究从一点到另外的点的最短路径算法已经很久了，也有很多的应用。简单的描述，从A点到B点有几条街道与路口，我们想要的就是找出A点到B点的最好捷径。有很多算法可以得到答案，每种算法也各有利弊。在我们深究他们之前，先考虑一下最幼稚的算法——也就是枚举所有的路径可能——需要多久的运行时间。如果把从A到B的所有可能路径都枚举一遍（不考虑回路绕圈圈），即使A与B只是在一个小镇上，在此生你我也都别想看到结果了。它的运行时间是与输入数据的指数级别成正比的。也就是时间复杂度为O(C^N )。即使C很小，其貌不扬的N都会让C^N 撑成天文数字。

解决这个问题的最快算法的运行时间可以达到O(E\*V\*Log(V))，E代表路(边)，V代表路口(点)。具象点说，这个算法可以在2秒内找到有10000个路口以及20000条街道的城市中找到一条最短路径。这个算法就是**迪科斯彻(Dijkstra)算法**。当然略复杂，需要用到数据结构的知识比如优先队列。但是在一些应用场景中，这还是太慢了(考虑纽约到旧金山有上千万的岔口)，程序员们试图用**启发式算法(heuristics)**来更好的解决这个问题。启发式算法是对问题的关联估算，经常由算法自身计算出来。在最短路径问题中，知道到目的地的距离估值是很有用的。知道了估值就可以应用更快的算法（A^* 算法有些时间就明显的更快于Djistra算法）这样程序员可以根据启发式搜索出这个值。这样的改往往并不会在最坏用例运行时间上得到优势，但是却能在现实世界中万马奔腾。

**近似算法**

有时候，无论再怎么牛逼立体的算法，甚至是最屌的启发式算法，在最快的计算机上运行还是嫌它慢。这时候如果牺牲掉结果的准确性就能提高运行速度。相对于找到最短路径，程序员可能找到比最短路径只长10%的路径就心满意足了。

实际上在许多重要的问题上，知名算法虽然提供了最佳答案但是都会因为难以忍受的慢而被弃用。这类问题最著名的就是NP问题，不确定多项式(Non-deterministic Polynomial)的缩写。当一个问题被描述为NP复杂以及NP难度的时候，这意味着没有人知道最理想的解决方法是什么。假如有一个家伙研究出了最有效解决一个NP难度的问题的算法，那么这个算法将会通吃所有的NP难度问题。

最典型最知名的NP难度问题是旅行商人问题(Traveling Salesman Problem)。一个商人要访问N个城市，并且他知道每个城市之间的距离。问题是“怎样最快访问所有的城市？”已知的解决这个问题的算法非常慢。程序员们还在寻找更快的并能给出虽然不是最好但是足够好的解决方案。

**随机算法**

另一种解决问题的方法是以某种方式随机一个算法。这么做无法帮助最坏用例的执行时间变快，但是可以提高用例平均执行时间。快排就是很好的利用了随机。快排在最坏的情况下需要O(N^2 )的时间复杂度。如果将随机整合进快排中，那么发生最坏情况的概率就会变的微乎其微，在平均执行时间上快排可以达到O(N\*Log(N))。当然其他算法可以保证在任何情况都是O(N*Log(N))，但是在平均执行时间上，快排是C\*N\*Log(N)，而其他算法则会是2\*C\*N\*Log(N)这样的节奏。

还有在查找中位数算法中，利用随机技巧可以使得时间复杂度仅为O(N)。而正常的做法往往是先一个排序然后再取出中位数，这得耗费O(N*Log(N))。这里必须说明的是，确定算法也可以达到O(N)，但是，随机往往有惊喜，常量C基本来说会比确定算法小很多。这种随机算法的基本思想就是随机挑出一个数，计算比它小或者等于它的数的个数，如果刚好是一半，恭喜你。a)如果这个个数假定为K，K小于N的1/2，那么中位数就是第(N/2-K)个比随机选定的数大的数。那么我们就把这K个数扔掉，现在的目标变成找剩余牌中第(N/2-K)小的那张牌。b)如果K大于N的1/2，那么中位数就是第(K-N/2)比随机选定的数小的数。那么我们就留下这K张牌，目标变成找留下的牌中第(K-N/2)大的牌。以此迭代就可找出中位数。计算其他第X位数的算法也同此如出一辙。

**压缩算法**

另一类的算法处理数据压缩的情景。这种算法没有固定的预期输出(与排序算法不同)，却在别的方面做优化。在数据压缩中，算法（例如LZW）尽量用最少的字节表示数据，并且在解压的时候能还原如初。但是在一些场景中，这种算法和其他的算法一样，会屈服于现实，结果是可用的，但是可能不是最佳的。JPG和MP3压缩，两者都是使得最终结果的品质低于原始的，但是却产生体积小的多的多的文件。MP3压缩没有把音乐原文件里的每个属性都保留，而是留下了主要的东西，并且把文件压缩成我们喜闻乐见的大小。JPG图片也是遵循这个原则，但图片与声音的策略略有不同。

**算法知识的重要性**

作为计算机科学家，只有理解了算法才能适当的应用它们。如果你在做一个软件的重要部分，你需要能评估它能运行多快。如果对算法的运行时间分析没有足够的了解，那么势必会导致评估不准确。总之你需要对涉及算法的细节了如指掌，这样你才能预测出软件不能快速运行以及正确运行的特殊情况。

当然，你也会遇到从未研究过的问题。这时候你需要想出一个新算法，或者把老算法用在新用途上。你知道的算法越多，你找到最佳解决方法的机会也越多。在许多场景下，新问题都能简化为老问题，你为了复用它必须得对老问题有基本认识。

来举个例子。因特网中的交换机。一个交换机有N根网线插着，从网线中收到数据包，交换机解析这些数据包，再从正确的网线中传输出去。交换机和电脑一样的，有时间片的概念，所以发送数据包也是间隔发送的。所以最好是在一个时间片段对着一个网线发送最多的数据包，并且能根据先来后到及时的发送所有数据包。所以这里面涉及的算法叫做“稳定匹配(stable matching)”，但第一眼可能觉得这两个东西并无关联。只有通过预先存在的算法知识，你才能发现这些关联。

**更多现实世界的例子**

现实世界的许多问题的解决方案都需要算法。基本上你使用的计算机就是依赖于许多前辈很辛苦很努力的研究各种算法终于研究出来的。现代计算机上最简单的应用如果没有算法也不能用——因为需要从硬盘加载数据以及管理内存。

有无数的应用需要复杂的算法。但是我接下来要谈及的两个问题需要与解决TopCoder问题一样的技巧。第一个就是众所周知的最大流问题(the maximun flow problem)，第二个是与动态规划(dynamic programming)相关的，一种有着闪电速度，经常用来解决看起来不可能完成任务的技术。

**最大流**

最大流问题是解决如何通过一个有序网络将东西从一个地方运送到另外一个的最好方案。更具体的来讲，这个问题首次出现是在20世纪50年代时，美国想知道苏联的铁路网能以多快的速度运送物资到东欧的卫星发射点。并且，美国想知道哪段铁路可以摧毁就可以阻断卫星发射点到整个苏联的通路。这两个问题是相关的，解决了最大流问题，也就解决了切断卫星发射点到苏联的最小代价的问题。

Ford和Fulkerson是首先找到有效算法的两位计算机科学家。所以算法的名字就理所当然的命名为Ford-Fulkerson算法，当然它也很著名咯。随后的50多年，更多的基于Ford-Fulkerson算法的改进算法被发明出来，当然它们更快。也更复杂。

这个问题出现后，更多的应用场景被挖掘出来。算法很明显的搭上了因特网，因为从一处到另一处尽可能多的数据传递是很重要的。它也衍生出了很多商业应用，是执行研究的重要组成。例如，你有N个雇员以及N份工作需要做，但并不是每个雇员能胜任每份工作，最大流就可以告知你如何分配恰到好处。SRM200的Graduation就是很好的利用最大流的例子。

**字符串比较**

许多程序员整个职业生涯都没有把动态规划引入代码中。但是动态规划在重要算法中榜上有名。有一种许多程序员用过，只是他们自己不知道的算法——在两个字符串中找不同。更特别些，它需要计算A转化B时所需要的插入，删除，编辑这些操作的最小值。

例如，考虑"AABAA"与"AAAB"。将第一个序列转化为第二个，最简单的做法是删除中间的B，然后将最后的A改成B。这种算法有包括DNA序列问题以及抄袭检测问题在内的应用场景。不管是SVN还是Git的版本比较都用到了这个算法。

如果没有动态规划，我们可能又要——你猜对了——一个天文数字的转换才能从一个字符串变为另外一个。但是动态规划将时间复杂度控制在了O(N*M)，N和M表示两个字符串的元素数量。

**总结**

人们学习的算法不同犹如他们解决的问题不同一样。然而，你试图解决的问题很可能与其他问题有相似之处。对于众多算法的理解有助于你可以挑选适合的算法解决问题。所以你在TopCoder上看到的竞赛以及问题都有助于你更好的掌握这些算法。虽然他们看起来不很切合实际，但是相同的算法知识是在现实世界中每天都被用到的。

























































